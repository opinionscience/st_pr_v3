{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 19:20:25.464182: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-18 19:20:25.495191: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 19:20:26.132364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from opsci_toolbox.helpers.common import load_pickle, write_pickle, load_csv, create_dir, read_json, write_json\n",
    "from opsci_toolbox.helpers.nlp import load_stopwords_df, load_spacy_model, TM_clean_text, PRarmy_nlp_process, load_HF_embeddings, HF_vectorize\n",
    "from opsci_toolbox.apis.webscraping import url_get_domain, parallel_twitter_oembed\n",
    "from opsci_toolbox.helpers.surreaction import *\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "project = \"TWITTER_LISTS_USERS_COMMUNS\"\n",
    "path = \"/home/erwan/scripts/st_pr_v3/notebooks/OUTPUTS\"\n",
    "\n",
    "config_file = \"/home/erwan/scripts/st_pr_v3/notebooks/configs/config_twitter_lists_users_communs.json\"\n",
    "\n",
    "# POUR FILTRER SUR UNE PERIODE DONNEE\n",
    "start_date = datetime(2024, 6, 1, tzinfo=pytz.UTC) \n",
    "end_date = datetime(2024, 7, 15, tzinfo=pytz.UTC)\n",
    "\n",
    "# SPACY CONFIG\n",
    "spacy_lang = \"en\"                                       #language of the stopwords\n",
    "spacy_model = \"en_core_web_lg\"                         # spacy model to import : ru_core_news_lg, en_core_web_lg, fr_core_news_lg\n",
    "pos_to_keep = [\"VERB\",\"NOUN\",\"ADJ\", \"ADV\", \"PROPN\"] \n",
    "entities_to_keep = ['PERSON','ORG', 'LOC']\n",
    "\n",
    "### VECTORISATION\n",
    "embedding_model = \"all-MiniLM-L6-v2\"        \n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {'batch_size':8}\n",
    "\n",
    "\n",
    "# CHEMIN VERS LES CORPUS\n",
    "# corpus_name = \"PATRIOTES\"\n",
    "# corpus_file = \"/home/erwan/scripts/bertopic/OUTPUTS/PR_army_v8/df_with_translatdion.pickle\"\n",
    "# twitter_corpus = \"/home/erwan/scripts/bertopic/OUTPUTS/PR_army_v8_twitter_listes_users_communs/df_with_translation.pickle\"\n",
    "# df_patriots = load_pickle(\"/home/erwan/scripts/bertopic/OUTPUTS/PR_army_v8_twitter/df_with_translation.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_json(os.path.join(path, config_file))\n",
    "path_project = create_dir(os.path.join(path, project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_478409/456018479.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['engagements'] = df[config['cols_engagements']].fillna(0).sum(axis=1)\n",
      "/tmp/ipykernel_478409/456018479.py:24: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(values[1]).astype(values[0])\n",
      "/tmp/ipykernel_478409/456018479.py:24: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(values[1]).astype(values[0])\n",
      "/tmp/ipykernel_478409/456018479.py:24: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(values[1]).astype(values[0])\n",
      "generation des index: 100%|██████████| 199562/199562 [00:04<00:00, 45081.40it/s]\n",
      "generation des index: 100%|██████████| 13957/13957 [00:00<00:00, 43209.19it/s]\n",
      "qualification des posts: 100%|██████████| 199562/199562 [00:04<00:00, 47709.38it/s]\n"
     ]
    }
   ],
   "source": [
    "df = load_pickle(config[\"corpus_file\"])\n",
    "\n",
    "# On supprime les doublons\n",
    "df = df.drop_duplicates(subset=config['col_id'])\n",
    "\n",
    "# FILTRE SUR LES DATES\n",
    "df = df[(df[config['col_date']] >= start_date) & (df[config['col_date']] <= end_date)]\n",
    "df['date'] = df[config['col_date']].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df.drop(columns=[config['col_date']], inplace = True)\n",
    "\n",
    "# on ajoute des colonnes\n",
    "df['plateforme'] = config['plateforme']\n",
    "df['corpus_name'] = config['corpus_name']\n",
    "\n",
    "# on calcule la somme des engagements\n",
    "df['engagements'] = df[config['cols_engagements']].fillna(0).sum(axis=1)\n",
    "\n",
    "# on ne conserve qu'une liste de colonnes correspondantes à celles du mapping\n",
    "cols_to_keep = list(config['col_mapping'].keys()) + ['plateforme', 'corpus_name', 'engagements', 'date'] \n",
    "df = df[cols_to_keep]\n",
    "\n",
    "# on rempli si vide et on change le type\n",
    "for col, values in config['col_mapping'].items():\n",
    "    df[col] = df[col].fillna(values[1]).astype(values[0])\n",
    "\n",
    "# on renomme les colonnes\n",
    "new_columns_names = {col: values[2] for col, values in config['col_mapping'].items()}\n",
    "df.rename(columns=new_columns_names, inplace=True)\n",
    "\n",
    "# on conserve uniquement les lignes avec du texte\n",
    "df = df[df['text'].str.len()>0]\n",
    "\n",
    "# on extrait le nom de domaine des URLs\n",
    "df[\"domain\"]=df[\"url\"].apply(url_get_domain)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "######################\n",
    "# SUREACTION\n",
    "######################\n",
    "\n",
    "df[config[\"cols_sureaction_metrics\"]]=df[config[\"cols_sureaction_metrics\"]].astype(int)\n",
    "\n",
    "df = avg_performance(\n",
    "    df, \n",
    "    col_date=\"date\", \n",
    "    col_author_id=\"user_id\", \n",
    "    col_engagement = config[\"cols_sureaction_metrics\"], \n",
    "    rolling_period= config[\"rolling_period_sureaction\"]\n",
    "    ) \n",
    "\n",
    "# on calcule les taux de sur-réaction pour notre liste de métriques\n",
    "df=kpi_reaction(df, config[\"cols_sureaction_metrics\"])\n",
    "cols_tx_engagement=['tx_'+c for c in config[\"cols_sureaction_metrics\"]]\n",
    "df[cols_tx_engagement]=df[cols_tx_engagement].fillna(-1)\n",
    "\n",
    "\n",
    "# on supprime nos colonnes contenant la performance moyenne (on ne devrait plus en avoir besoin)\n",
    "cols_to_drop = [c for c in df.columns if c.lower()[-4:] == '_avg']\n",
    "cols_to_drop += ['index']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# on catégorise les formes de réaction\n",
    "cols_typologie = [\"tx_\"+ col for col in config[\"cols_typologie_sureaction\"]]\n",
    "df=get_reactions_type(df, cols_typologie, 'type_engagement')\n",
    "\n",
    "df.drop(columns=cols_typologie, inplace=True)\n",
    "\n",
    "df.to_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des Embed Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' not in locals():\n",
    "    df = pd.read_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))\n",
    "\n",
    "if config['plateforme']==\"Twitter\":\n",
    "    usernames = df['user_name'].tolist()\n",
    "    tweet_ids = df['message_id'].tolist()\n",
    "    df_embed = parallel_twitter_oembed(usernames, tweet_ids, omit_script=True)\n",
    "    df = pd.merge(df, df_embed, on=['user_name',\"message_id\"], how='left')\n",
    "else:\n",
    "    df[\"tweet_html\"] = None\n",
    "\n",
    "df.to_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation & NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT PRECLEANING\n",
      "NLP PROCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLP Process: 100%|██████████| 199562/199562 [04:12<00:00, 789.76it/s] \n"
     ]
    }
   ],
   "source": [
    "if 'df' not in locals():\n",
    "    df = pd.read_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))\n",
    "\n",
    "df_stopwords = load_stopwords_df(spacy_lang)\n",
    "stopwords = df_stopwords['word'].to_list()\n",
    "\n",
    "nlp = load_spacy_model(spacy_model,  disable_components=[\"transformer\", \"trainable_lemmatizer\", \"textcat_multilabel\", \"textcat\", \"entity_ruler\", \"entity_linker\"], lang_detect=False, emoji=True)\n",
    "\n",
    "# basic precleaning of text \n",
    "print(\"TEXT PRECLEANING\")\n",
    "df = TM_clean_text(df, \"translated_text\", \"clean_text\")\n",
    "\n",
    "# lemmatize text, remove stop words and keep only some PoS\n",
    "print(\"NLP PROCESS\")\n",
    "df = PRarmy_nlp_process(nlp, df, \"clean_text\", \"lemmatized_text\", pos_to_keep, entities_to_keep, stopwords, batch_size=100, n_process=1) \n",
    "\n",
    "df.drop(columns=['clean_text'], inplace=True)\n",
    "\n",
    "df.to_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' not in locals():\n",
    "    df = pd.read_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))\n",
    "\n",
    "HF_encoder = load_HF_embeddings(embedding_model, encode_kwargs, model_kwargs)\n",
    "\n",
    "embeddings = HF_vectorize(HF_encoder, list(df[\"translated_text\"]))\n",
    "\n",
    "df[\"embeddings\"]=embeddings\n",
    "df.to_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False) \n",
    "\n",
    "# embeddings = model.encode(list(df[\"translated_text\"]), \n",
    "#                             batch_size=8, \n",
    "#                             max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "#                             )['dense_vecs']\n",
    "\n",
    "# df[\"embeddings\"]=embeddings.tolist()\n",
    "# df.to_parquet(os.path.join(path_project, f'{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}_{config[\"plateforme\"]}_{config[\"corpus_name\"]}.parquet'))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
